---
layout: post
title: KNN (K-Nearest Neighbors) from Scratch
---
# How does KNN classifier work?
![Figure 1](/img/KNN_image.png)
KNN classifier algorithm works on a very simple principle. Let’s explain briefly in Figure above. We have an entire dataset with 2 labels, Class A and Class B. Class A belongs to the yellow data and Class B belongs to the purple data. While predicting, it compares the input (red star) to the entire existing data and checks the similarity between them. What happens when we take k Nearest Neighbors k=3? What happens with k=6? With k=3 — two data points belong to a purple class and one belongs to the yellow class. The majority vote is purple, so the returned predicted output is Class B. But when we have our k nearest neighbors equal to six (k=6), four data points belong to the yellow class and two belong to the purple class. So majority votes are yellow so our predicted label will be Class A. Easy enough right?!!

Maybe these real-world examples will help:

* Marketing: imagine you purchased a baby stroller on Amazon, the next day you receive an email from the company that you might be interested in a set of pacifiers. Amazon can target you with similar products, or products with other customers that have similar buying habits as you.

* Content recommendation: maybe the most interesting example is Spotify and its acclaimed recommendation engine. KNN is used on many content recommendation engines, more powerful systems are now available (auto-encoders), but KNN is an example of how Spotify could use this implementation.

# Diving into the Code
In order to calculate what we did above. We need a distance function. There are many distance functions to choose from. In this case, we will be using the famous Iris DataSet from SKlearn to test our algorithm built from scratch, so we will use the Euclidean distance. It is calculated as follows:

